# -*- coding: utf-8 -*-
"""lightning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mw0EJLryiSrsNB1gF36Vd8dGK2X-icep
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install lightning pydicom -q

# %% imports
import os
from pathlib import Path

import torch
from torch import nn
import torch.nn.functional as F
from torchvision import transforms
from torchvision.transforms import v2
from torchvision.datasets import MNIST, utils, VisionDataset
from torch.utils.data import DataLoader, random_split
import lightning as L
import pandas as pd
from torch.utils.data import Dataset
import matplotlib.pyplot as plt
import numpy as np
import pydicom


torch.set_float32_matmul_precision('medium')

def read_image(path):
  dcm = pydicom.dcmread(path)
  return dcm.pixel_array + int(dcm.RescaleIntercept)

def get_patch(img, target, patch_size):
  patched_img = img.unfold(0, patch_size, patch_size//2).unfold(1, patch_size, patch_size//2)
  patched_target = target.unfold(0, patch_size, patch_size//2).unfold(1, patch_size, patch_size//2)
  ix, iy = torch.randint(0, patched_img.shape[0]-1, size=(2, 1))
  return patched_img[ix, iy], patched_target[ix, iy]


class PediatricIQDataset(VisionDataset):
    def __init__(self,
                 root=os.getcwd(),
                 train: bool=True,
                 transform=None,
                 target_transform=None,
                 download=True,
                 patch_n=None,
                 patch_size=None,
                 testid=7):

      peds_datadir = Path(root) / 'pediatricIQphantoms'
      if download & (not peds_datadir.exists()):
        utils.download_and_extract_archive(url='https://zenodo.org/records/11267694/files/pediatricIQphantoms.zip',
                                           download_root=root)
      df = pd.read_csv(peds_datadir / 'metadata.csv')
      metadata = df[(df['pediatric subgroup'] == 'adult') &
                    (df['series'] == 'simulation')]
      if train:
        metadata = metadata[metadata['patientid'] != testid]
      else:
        metadata = metadata[metadata['patientid'] == testid]
      self.root = peds_datadir
      self.metadata = metadata
      self.patch_n = patch_n
      self.patch_size = patch_size
      self.ld_metadata = self.metadata[self.metadata['Dose [%]'] == 25]
      self.rd_metadata = self.metadata[self.metadata['Dose [%]'] == 100]
      self.transform = transform
      self.target_transform = target_transform

    def __len__(self):
      return len(self.ld_metadata)

    def __getitem__(self, idx):
      ld_patient = self.ld_metadata.iloc[idx]
      ld_img_path = self.root / ld_patient['file']
      image = read_image(ld_img_path)

      rd_patient = self.rd_metadata[(self.rd_metadata['patientid'] == ld_patient['patientid']) &
                                    (self.rd_metadata['repeat'] == ld_patient['repeat'])]
      rd_img_path = self.root / rd_patient['file'].item()
      label = read_image(rd_img_path)
      if self.transform:
        image = self.transform(image)
      if self.target_transform:
        label = self.target_transform(label)

      if self.patch_size:
        image, label = get_patch(image.squeeze(),
                                 label.squeeze(),
                                  self.patch_size)
      return image, label

"""Next turn this into a [pytorch dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)"""

class PediatricIQDataModule(L.LightningDataModule):
    def __init__(self, data_dir: str = "./", patch_size=64, batch_size=32):
        super().__init__()
        self.data_dir = data_dir
        self.patch_size = patch_size
        self.batch_size = batch_size
        self.transform = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=False)])

    def prepare_data(self):
        # download
        PediatricIQDataset(self.data_dir, train=True, patch_size=self.patch_size,
                           transform=transforms.ToTensor(), target_transform=transforms.ToTensor())


    def setup(self, stage: str):
        # Assign train/val datasets for use in dataloaders
        if stage == "fit":
          train_set = PediatricIQDataset(saved_path, train=True, patch_size=self.patch_size,
                                        transform=self.transform, target_transform=self.transform)
            # use 20% of training data for validation
          train_set_size = int(len(train_set) * 0.8)
          valid_set_size = len(train_set) - train_set_size

          # split the train set into two
          seed = torch.Generator().manual_seed(42)
          self.train_set, self.val_set = random_split(train_set, [train_set_size,
                                                        valid_set_size],
                                                        generator=seed)

        # Assign test dataset for use in dataloader(s)
        if stage == "test":
          self.test_set = PediatricIQDataset(saved_path, train=False,
                               transform=self.transform, target_transform=self.transform)

        if stage == "predict":
          self.predict_set = PediatricIQDataset(saved_path, train=False,
                                                transform=self.transform, target_transform=self.transform)

    def train_dataloader(self):
      return DataLoader(self.train_set, batch_size=self.batch_size)

    def val_dataloader(self):
      return DataLoader(self.val_set, batch_size=self.batch_size)

    def test_dataloader(self):
      return DataLoader(self.test_set, batch_size=self.batch_size)

    def predict_dataloader(self):
      return DataLoader(self.predict_set, batch_size=self.batch_size)

saved_path='data'
dm = PediatricIQDataModule(saved_path)
dm.prepare_data()
dm.setup('fit')
dl = dm.train_dataloader()

for x, y in dl:
  print(x.shape, y.shape)
  break
idx = 30
f, axs = plt.subplots(1, 2, dpi=150)
axs[0].imshow(x[idx].squeeze(), cmap='gray')
axs[1].imshow(y[idx].squeeze(), cmap='gray')

import os
import numpy as np
import torch.nn as nn
import torch
from tqdm import tqdm

class RED_CNN(nn.Module):
    def __init__(self, out_ch=96, norm_range_min=-1024, norm_range_max=3072):
        super(RED_CNN, self).__init__()
        self.norm_range_min = norm_range_min
        self.norm_range_max = norm_range_max
        self.conv1 = nn.Conv2d(1, out_ch, kernel_size=5, stride=1, padding=0)
        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=5, stride=1, padding=0)
        self.conv3 = nn.Conv2d(out_ch, out_ch, kernel_size=5, stride=1, padding=0)
        self.conv4 = nn.Conv2d(out_ch, out_ch, kernel_size=5, stride=1, padding=0)
        self.conv5 = nn.Conv2d(out_ch, out_ch, kernel_size=5, stride=1, padding=0)

        self.tconv1 = nn.ConvTranspose2d(out_ch, out_ch, kernel_size=5, stride=1, padding=0)
        self.tconv2 = nn.ConvTranspose2d(out_ch, out_ch, kernel_size=5, stride=1, padding=0)
        self.tconv3 = nn.ConvTranspose2d(out_ch, out_ch, kernel_size=5, stride=1, padding=0)
        self.tconv4 = nn.ConvTranspose2d(out_ch, out_ch, kernel_size=5, stride=1, padding=0)
        self.tconv5 = nn.ConvTranspose2d(out_ch, 1, kernel_size=5, stride=1, padding=0)

        self.relu = nn.ReLU()

    def forward(self, x):
        # encoder
        x = self.normalize(x)
        residual_1 = x
        out = self.relu(self.conv1(x))
        out = self.relu(self.conv2(out))
        residual_2 = out
        out = self.relu(self.conv3(out))
        out = self.relu(self.conv4(out))
        residual_3 = out
        out = self.relu(self.conv5(out))
        # decoder
        out = self.tconv1(out)
        out += residual_3
        out = self.tconv2(self.relu(out))
        out = self.tconv3(self.relu(out))
        out += residual_2
        out = self.tconv4(self.relu(out))
        out = self.tconv5(self.relu(out))
        out += residual_1
        out = self.relu(out)
        out = self.denormalize(out)
        return out

    def normalize(self, image):
        image = (image - self.norm_range_min) / (self.norm_range_max - self.norm_range_min)
        return image

    def denormalize(self, image):
        image = image * (self.norm_range_max - self.norm_range_min) + self.norm_range_min
        return image

    def predict(self, image, batch_size=1, device='cpu'):
        if image.ndim == 2: image = image[None, None, :, :]
        if image.dtype != 'float32': image = image.astype('float32')
        with torch.no_grad():
            n_images = image.shape[0]
            if batch_size > n_images:
                batch_size = n_images
            batch_indices = np.arange(n_images)
            if n_images % batch_size == 0:
                batch_indices = np.split(batch_indices, n_images//batch_size)
                if batch_size == 1: batch_indices = np.array(batch_indices).reshape(-1, 1)
            else:
                modulo=n_images % batch_size
                batch_indices = np.split(batch_indices[:n_images-modulo], n_images//batch_size)
                batch_indices.append(list(range(n_images-modulo, n_images)))
            pred = np.zeros_like(image)
            image = torch.tensor(image, device=device)
            for batch in tqdm(batch_indices):
                pred[batch] = self.forward(image[batch]).to('cpu').numpy()
            return pred

class LitAutoEncoder(L.LightningModule):
    def __init__(self, torch_module):
        super().__init__()
        self.torch_module = torch_module

    def training_step(self, batch, batch_idx):
      # training_step defines the train loop.
      x, y = batch
      x_hat = self.torch_module(x)
      loss = F.mse_loss(x_hat, y)
      self.log('train_loss', loss)
      return loss

    def validation_step(self, batch, batch_idx):
      # training_step defines the train loop.
      x, y = batch
      x_hat = self.torch_module(x)
      val_loss = F.mse_loss(x_hat, y)
      self.log("val_loss", val_loss, prog_bar=True)

    def test_step(self, batch, batch_idx):
      # this is the test loop
      x, y = batch
      x_hat = self.torch_module(x)
      test_loss = F.mse_loss(x_hat, y)
      self.log("test_loss", test_loss)

    def forward(self, x):
      return self.torch_module(x)

    def predict_step(self, batch, batch_idx=None, dataloader_idx=0):
      return self(batch)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)
        return optimizer

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
# %tensorboard --logdir=lightning_logs/

dm = PediatricIQDataModule(saved_path)
model = LitAutoEncoder(RED_CNN())
trainer = L.Trainer(max_epochs=100)
trainer.fit(model, datamodule=dm)
trainer.test(datamodule=dm)
trainer.validate(datamodule=dm)
trainer.predict(datamodule=dm)

# %% test before fitting
pre_test = trainer.predict(datamodule=dm)
pre_test

idx = 6
f, axs = plt.subplots(1, 3, dpi=150)
trainer = L.Trainer()
x = torch.tensor(test_set[0][0][None])
res = trainer.predict(autoencoder, x)
xhat = res[0].detach().numpy()[0]
axs[0].imshow(x.squeeze(), cmap='gray', vmin=vmin, vmax=vmax)
axs[1].imshow(xhat, cmap='gray', vmin=vmin, vmax=vmax)
axs[2].imshow(x.squeeze() - xhat, cmap='gray', vmin=vmin, vmax=vmax)



# train model
trainer = L.Trainer(max_epochs=50)
trainer.fit(model=autoencoder, train_dataloaders=train_loader,
            val_dataloaders=valid_loader)

# %% test after fitting
post_test = trainer.test(model=autoencoder, dataloaders=DataLoader(test_set))
post_test
# %%

import matplotlib.pyplot as plt
test_set[0][0]

trainer = L.Trainer()
x = torch.tensor(test_set[10][0][None])
res = trainer.predict(autoencoder, x)

idx = 6
f, axs = plt.subplots(1, 3, dpi=150)
xhat = res[0].detach().numpy()[0]
axs[0].imshow(x.squeeze(), cmap='gray', vmin=vmin, vmax=vmax)
axs[1].imshow(xhat, cmap='gray', vmin=vmin, vmax=vmax)
axs[2].imshow(x.squeeze() - xhat, cmap='gray', vmin=vmin, vmax=vmax)

